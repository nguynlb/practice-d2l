{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/nguynlb/practice-d2l/blob/master/ComputerVision/ImageSegmentation/UNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FHzt57DhXngF",
    "outputId": "a25b895f-a3d8-4727-d990-83a365f869bc"
   },
   "outputs": [],
   "source": [
    "!pip3 -q install albumentations\n",
    "!pip3 -q install torchmetrics\n",
    "!pip3 -q install opencv-python\n",
    "!pip3 -q install segmentation_models_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ah_J6F9CXaTV"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import torchmetrics\n",
    "from torchmetrics import Dice, JaccardIndex\n",
    "import segmentation_models_pytorch as smp\n",
    "from typing import Tuple, Dict, List\n",
    "from pathlib import Path\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NlFnW9ylYoEy",
    "outputId": "4af56c48-2704-4a6e-b620-8c85a7e65acd"
   },
   "outputs": [],
   "source": [
    "def download_dataset():\n",
    "    if 'google.colab' in str(get_ipython()):\n",
    "        !wget https://thor.robots.ox.ac.uk/~vgg/data/pets/images.tar.gz\n",
    "        !wget https://thor.robots.ox.ac.uk/~vgg/data/pets/annotations.tar.gz\n",
    "            \n",
    "        !tar -xf images.tar.gz\n",
    "        !tar -xf annotations.tar.gz\n",
    "    else:\n",
    "        !python -m wget https://thor.robots.ox.ac.uk/~vgg/data/pets/images.tar.gz  -o images.tar.gz\n",
    "        !python -m wget https://thor.robots.ox.ac.uk/~vgg/data/pets/annotations.tar.gz -o annotations.tar.gz\n",
    "        import tarfile\n",
    "        import os \n",
    "        cur_dir = os.getcwd()\n",
    "        with tarfile.open(\"images.tar.gz\") as f:\n",
    "          f.extractall(path=f\"{cur_dir}/data\")\n",
    "        with tarfile.open(\"annotations.tar.gz\") as f:\n",
    "          f.extractall(path=f\"{cur_dir}/data\")\n",
    "        print(\"Done...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CW5KkKTbY0ku"
   },
   "outputs": [],
   "source": [
    "data_path = Path(\"data\")\n",
    "if data_path.is_dir():\n",
    "    print(f\"Dataset has been downloaded. Skip downloading...\")\n",
    "else:\n",
    "    download_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A2ZzT1PzaRL_"
   },
   "source": [
    "# Plot image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384
    },
    "id": "VVaTrtxeY90Z",
    "outputId": "60b13aaa-3677-4bfb-8c65-9eab0d30099c"
   },
   "outputs": [],
   "source": [
    "cur_dir = os.getcwd()\n",
    "\n",
    "image_path = os.path.join(cur_dir,\"data\", \"images\", \"Abyssinian_1.jpg\")\n",
    "image = cv2.imread(image_path)\n",
    "image_cvt = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.imshow(image_cvt)\n",
    "plt.axis(False)\n",
    "plt.show();\n",
    "\n",
    "print(f\"Shape of image: {image_cvt.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384
    },
    "id": "O89InyWDZuR_",
    "outputId": "193e9b6c-dc19-4c89-d8b6-6c17b10a56d8"
   },
   "outputs": [],
   "source": [
    "mark_path = os.path.join(cur_dir,\"data\", \"annotations\",\"trimaps\",\"Abyssinian_1.png\")\n",
    "mark = cv2.imread(mark_path)\n",
    "mark = cv2.cvtColor(mark, cv2.COLOR_BGR2GRAY)\n",
    "plt.imshow(mark);\n",
    "plt.axis(False)\n",
    "plt.show();\n",
    "\n",
    "print(f\"Shape of mark: {mark.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YLsCKMYobSro"
   },
   "source": [
    "# Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TgXHO_Z_bUcQ"
   },
   "outputs": [],
   "source": [
    "class DogCatDataset(Dataset):\n",
    "  def __init__(self, root_dir : str ,txt_file : str, transform : A.Compose = None) -> None:\n",
    "    super().__init__()\n",
    "\n",
    "    self.root_dir = root_dir\n",
    "    self.txt_file = txt_file\n",
    "    self.transform = transform\n",
    "    self.list_image_path = []\n",
    "    with open(txt_file, \"r\") as file:\n",
    "      for line in file:\n",
    "        self.list_image_path.append(line.split()[0])\n",
    "\n",
    "  def __len__(self) -> int:\n",
    "    return len(self.list_image_path)\n",
    "\n",
    "  def __getitem__(self, idx: int) -> Tuple[torch.Tensor]:\n",
    "    # Read image\n",
    "    image_path = os.path.join(self.root_dir, \"images\", f\"{self.list_image_path[idx]}.jpg\")\n",
    "    image = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Read mark\n",
    "    mask_path = os.path.join(self.root_dir, \"annotations\", \"trimaps\" ,f\"{self.list_image_path[idx]}.png\")\n",
    "    mask = cv2.cvtColor(cv2.imread(mask_path), cv2.COLOR_BGR2GRAY)\n",
    "    mask[mask == 2] = 0\n",
    "    mask[mask == 3] = 1\n",
    "\n",
    "    if self.transform is not None:\n",
    "      transformed = self.transform(image=image, mask=mask)\n",
    "      image = transformed['image']\n",
    "      mask = transformed['mask']\n",
    "\n",
    "    return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"data\"\n",
    "txt_file = \"data/annotations/trainval.txt\"\n",
    "dataset = DogCatDataset(root_dir=root_dir, txt_file=txt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize_image(image: torch.Tensor) -> torch.Tensor:\n",
    "  mean, std = (0.485, 0.456, 0.406), (0.229, 0.224, 0.225)\n",
    "\n",
    "  for i, m, s in zip(image, mean, std):\n",
    "    i.mul_(s).add_(m)\n",
    "\n",
    "\n",
    "  return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_image_mark(dataset: DogCatDataset, n : int = 3,  seed: int = 82) -> None:\n",
    "  \"\"\"Visualize n-random picture and segmentation\n",
    "  Args:\n",
    "    - dataset: dataset want to visualize\n",
    "    - n: number of picture\n",
    "    - seed: seed random\n",
    "  \"\"\"\n",
    "  np.random.seed(seed)\n",
    "  size = len(dataset)\n",
    "\n",
    "  fig, ax = plt.subplots(n, 2, figsize=(6, 6))\n",
    "\n",
    "  for i in range(n):\n",
    "    rand_idx = np.random.randint(size)\n",
    "    image, mark = dataset[rand_idx]\n",
    "\n",
    "    if isinstance(image, torch.Tensor):\n",
    "      image = denormalize_image(image)\n",
    "      image = image.permute(1, 2, 0)\n",
    "\n",
    "    ax0 = ax[i, 0]\n",
    "    ax0.imshow(image)\n",
    "    ax0.set_title(\"Image\")\n",
    "    ax0.get_xaxis().set_visible(False)\n",
    "    ax0.get_yaxis().set_visible(False)\n",
    "\n",
    "    ax1 = ax[i, 1]\n",
    "    ax1.imshow(mark)\n",
    "    ax1.set_title(\"Segmentation\")\n",
    "    ax1.get_xaxis().set_visible(False)\n",
    "    ax1.get_yaxis().set_visible(False)\n",
    "\n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_image_mark(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_size = 384\n",
    "\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(width=transform_size, height=transform_size),\n",
    "    A.HorizontalFlip(),\n",
    "    A.RandomBrightnessContrast(),\n",
    "    A.Blur(),\n",
    "    A.Cutout(num_holes=8, max_h_size=8, max_w_size=8, fill_value=0, always_apply=False, p=0.5),\n",
    "    A.Normalize(),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "test_transform = A.Compose([\n",
    "    A.Resize(width=transform_size, height=transform_size),\n",
    "    A.Normalize(),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "root_dir = \"/data\"\n",
    "train_file = \"/data/annotations/trainval.txt\"\n",
    "test_file = \"/data/annotations/test.txt\"\n",
    "\n",
    "train_dataset = DogCatDataset(root_dir=root_dir,\n",
    "                              txt_file=train_file,\n",
    "                              transform=train_transform)\n",
    "\n",
    "test_dataset = DogCatDataset(root_dir=root_dir,\n",
    "                             txt_file=test_file,\n",
    "                             transform=test_transform)\n",
    "\n",
    "\n",
    "visualize_image_mark(dataset=train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_workers = 4\n",
    "\n",
    "train_dataloader = DataLoader(dataset=train_dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              num_workers=num_workers,\n",
    "                              shuffle=True)\n",
    "\n",
    "test_dataloader = DataLoader(dataset=test_dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              num_workers=num_workers,\n",
    "                              shuffle=False)\n",
    "\n",
    "batch_image, batch_mask = next(iter(train_dataloader))\n",
    "batch_image.shape, batch_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "  def __init__(self, n_class: int, in_channels: int = 3, hidden_channels: List[int] = [64, 128, 256, 512, 1025]) -> None:\n",
    "    super(UNet, self).__init__()\n",
    "\n",
    "    if (len(hidden_channels) != 5):\n",
    "      raise AttributeError(f\"\"\"\n",
    "      UNet is implemented with 4 depth layers and 1 latent space.\n",
    "      Size of hidden_channels must be 5. Find {len(hidden_channels)}\n",
    "      \"\"\")\n",
    "\n",
    "    def conv2d_block(in_channels, out_channels) -> nn.Module:\n",
    "      return nn.Sequential(\n",
    "          nn.Conv2d(in_channels, out_channels, 3, padding=\"same\"),\n",
    "          nn.ReLU(),\n",
    "          nn.Conv2d(out_channels, out_channels, 3, padding=\"same\"),\n",
    "          nn.ReLU(),\n",
    "      )\n",
    "\n",
    "\n",
    "    self.n_class = n_class\n",
    "\n",
    "    # Encoder block\n",
    "    self.down_sample = nn.MaxPool2d(2)\n",
    "    self.downblock_0 = conv2d_block(in_channels, hidden_channels[0])\n",
    "    self.downblock_1 = conv2d_block(hidden_channels[0], hidden_channels[1])\n",
    "    self.downblock_2 = conv2d_block(hidden_channels[1], hidden_channels[2])\n",
    "    self.downblock_3 = conv2d_block(hidden_channels[2], hidden_channels[3])\n",
    "\n",
    "    # Latent Space\n",
    "    self.latent_block = conv2d_block(hidden_channels[3], hidden_channels[4])\n",
    "\n",
    "    # Decoder block\n",
    "    self.up_sample = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "    self.upblock_3 = conv2d_block(hidden_channels[4] + hidden_channels[3], hidden_channels[3])\n",
    "    self.upblock_2 = conv2d_block(hidden_channels[3] + hidden_channels[2], hidden_channels[2])\n",
    "    self.upblock_1 = conv2d_block(hidden_channels[2] + hidden_channels[1], hidden_channels[1])\n",
    "    self.upblock_0 = conv2d_block(hidden_channels[1] + hidden_channels[0], hidden_channels[0])\n",
    "\n",
    "    # Output\n",
    "    self.output = nn.Conv2d(hidden_channels[0], self.n_class, 1, 1, 1)\n",
    "\n",
    "  def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
    "    # Encoder Block 0\n",
    "    x0 = self.downblock_0(x)\n",
    "    x = self.down_sample(x0)\n",
    "\n",
    "    # Encoder Block 1\n",
    "    x1 = self.downblock_1(x)\n",
    "    x = self.down_sample(x1)\n",
    "\n",
    "    # Encoder Block 2\n",
    "    x2 = self.downblock_2(x)\n",
    "    x = self.down_sample(x2)\n",
    "\n",
    "    # Encoder Block 3\n",
    "    x3 = self.downblock_3(x)\n",
    "    x = self.down_sample(x3)\n",
    "\n",
    "    # Latent Space\n",
    "    x = self.latent_block(x)\n",
    "\n",
    "    # Decoder Block 3\n",
    "    x = self.up_sample(x)\n",
    "    x = torch.cat((x3, x), dim = 1)\n",
    "    x = self.upblock_3(x)\n",
    "\n",
    "    # Decoder Block 2\n",
    "    x = self.up_sample(x)\n",
    "    x = torch.cat((x2, x), dim = 1)\n",
    "    x = self.upblock_2(x)\n",
    "\n",
    "    # Decoder Block 1\n",
    "    x = self.up_sample(x)\n",
    "    x = torch.cat((x1, x), dim = 1)\n",
    "    x = self.upblock_1(x)\n",
    "\n",
    "    # Decoder Block 0\n",
    "    x = self.up_sample(x)\n",
    "    x = torch.cat((x0, x), dim = 1)\n",
    "    x = self.upblock_0(x)\n",
    "\n",
    "    return self.output(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "unet = UNet(n_class = 2, in_channels = 3).to(device)\n",
    "\n",
    "batch_image = batch_image.to(device)\n",
    "unet(batch_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyP/qdce4+KjX6DIEqMgonmf",
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
